"""
Enhanced Scale Detection - Split Bar Method with SEM Parameter Filtering
Handles scale bars that are split by the text overlay

Key insight: Scale bar = left_segment + text_width + right_segment
The line should be at text middle height and aligned horizontally.
"""

import cv2
import numpy as np
import re
from typing import Dict, Optional, List, Tuple
from collections import Counter

# EasyOCR import
try:
    import easyocr
    EASYOCR_AVAILABLE = True
except:
    EASYOCR_AVAILABLE = False

def detect_scale_bar_split_aware(image_input, debug=True, **kwargs) -> Dict:
    """
    Enhanced scale detection that properly handles split scale bars.
    
    Method:
    1. Find scale text with precise bounding box
    2. Look for line segments on left and right of text at text's vertical middle
    3. Calculate total span: left_line + text_width + right_line
    """
    
    # Handle image loading
    if isinstance(image_input, str):
        image = cv2.imread(image_input, cv2.IMREAD_GRAYSCALE)
        if image is None:
            return {
                'scale_detected': False,
                'micrometers_per_pixel': 0.0,
                'scale_factor': 0.0,
                'error': f'Could not load image: {image_input}'
            }
    else:
        image = image_input
    
    if debug:
        print(f"üîç Analyzing image: {image.shape}")
    
    result = {
        'scale_detected': False,
        'micrometers_per_pixel': 0.0,
        'scale_factor': 0.0,
        'error': None,
        'method_used': 'split_bar_aware'
    }
    
    # Step 1: Find scale text with precise location
    if debug:
        print("üìù Step 1: Finding scale text with precise bounds...")
    
    text_info = find_scale_text_precise(image, debug)
    
    if not text_info:
        result['error'] = "No scale text found"
        return result
    
    if debug:
        print(f"‚úÖ Found text: '{text_info['text']}' = {text_info['micrometers']} Œºm")
        print(f"   Text bbox: {text_info['bbox']}")
        print(f"   Text center: ({text_info['center_x']}, {text_info['center_y']})")
        print(f"   Text width: {text_info['width']} pixels")
    
    # Step 2: Find split scale bar segments
    if debug:
        print("üìè Step 2: Finding split scale bar segments...")
    
    total_span = find_split_scale_bar(image, text_info, debug)
    
    if total_span <= 0:
        result['error'] = "Could not detect complete scale bar span"
        return result
    
    # Step 3: Calculate scale factor
    micrometers_per_pixel = text_info['micrometers'] / total_span
    
    result.update({
        'scale_detected': True,
        'micrometers_per_pixel': micrometers_per_pixel,
        'scale_factor': micrometers_per_pixel,
        'scale_info': text_info,
        'bar_length_pixels': total_span,
        'detection_method': 'split_bar_reconstruction'
    })
    
    if debug:
        print(f"‚úÖ SUCCESS: {micrometers_per_pixel:.4f} Œºm/pixel")
        print(f"   Total span: {total_span} pixels")
    
    return result

def find_scale_text_precise(image: np.ndarray, debug=False) -> Optional[Dict]:
    """
    Find scale text with precise bounding box information.
    Enhanced to filter out SEM parameter text and focus on actual scale bars.
    """
    
    if not EASYOCR_AVAILABLE:
        if debug:
            print("‚ùå EasyOCR not available")
        return None
    
    try:
        reader = easyocr.Reader(['en'], verbose=False)
        
        # Search bottom 30% of image
        height, width = image.shape
        search_height = int(height * 0.3)
        bottom_region = image[height - search_height:, :]
        y_offset = height - search_height
        
        if debug:
            print(f"   Searching bottom region: {bottom_region.shape}")
        
        # Try multiple OCR parameter settings for better detection
        all_results = []
        
        # Standard detection
        results = reader.readtext(bottom_region, detail=1)
        all_results.extend(results)
        
        # Also try with lower text threshold for difficult text
        try:
            results_low_thresh = reader.readtext(bottom_region, detail=1, text_threshold=0.5)
            all_results.extend(results_low_thresh)
        except:
            pass
        
        # Also try with preprocessing
        try:
            # Enhance contrast for better OCR
            enhanced = cv2.convertScaleAbs(bottom_region, alpha=1.5, beta=20)
            results_enhanced = reader.readtext(enhanced, detail=1)
            all_results.extend(results_enhanced)
        except:
            pass
        
        # Remove duplicates based on text content
        seen_texts = set()
        unique_results = []
        for result in all_results:
            text = result[1].strip()
            if text not in seen_texts:
                seen_texts.add(text)
                unique_results.append(result)
        
        if debug:
            print(f"   EasyOCR found {len(unique_results)} unique text elements")
            # Show ALL detected text, even low confidence
            for (bbox, text, confidence) in unique_results:
                print(f"     '{text}' (confidence: {confidence:.3f})")
        
        # Collect all scale candidates
        scale_candidates = []
        
        # Process each text detection with a LOWER confidence threshold
        for (bbox, text, confidence) in unique_results:
            if confidence > 0.1:  # Lower threshold to catch more candidates
                if debug and confidence <= 0.3:
                    print(f"     [Low confidence] '{text}' (confidence: {confidence:.3f})")
                
                # FILTER OUT SEM PARAMETERS - this is the key enhancement!
                if is_sem_parameter_text(text):
                    if debug:
                        print(f"       ‚ùå Filtered out (SEM parameter)")
                    continue
                
                scale_info = parse_scale_text_flexible(text.strip())
                if scale_info:
                    # Calculate precise bounding box in full image coordinates
                    bbox_array = np.array(bbox)
                    
                    # Adjust bbox to full image coordinates
                    bbox_adjusted = bbox_array.copy()
                    bbox_adjusted[:, 1] += y_offset
                    
                    # Calculate precise dimensions
                    min_x = int(np.min(bbox_adjusted[:, 0]))
                    max_x = int(np.max(bbox_adjusted[:, 0]))
                    min_y = int(np.min(bbox_adjusted[:, 1]))
                    max_y = int(np.max(bbox_adjusted[:, 1]))
                    
                    center_x = (min_x + max_x) // 2
                    center_y = (min_y + max_y) // 2
                    text_width = max_x - min_x
                    text_height = max_y - min_y
                    
                    # Additional validation for scale text
                    validation_score = validate_scale_text_context(text, bbox_adjusted, image.shape)
                    
                    if debug:
                        print(f"       ‚úÖ Parsed: {scale_info['value']} {scale_info['unit']} = {scale_info['micrometers']} Œºm")
                        print(f"       Text dimensions: {text_width}x{text_height} pixels")
                        print(f"       Validation score: {validation_score:.3f}")
                    
                    scale_candidates.append({
                        'text': text.strip(),
                        'value': scale_info['value'],
                        'unit': scale_info['unit'],
                        'micrometers': scale_info['micrometers'],
                        'confidence': confidence,
                        'center_x': center_x,
                        'center_y': center_y,
                        'bbox': bbox_adjusted,
                        'min_x': min_x,
                        'max_x': max_x,
                        'min_y': min_y,
                        'max_y': max_y,
                        'width': text_width,
                        'height': text_height,
                        'original_text': text.strip(),
                        'validation_score': validation_score
                    })
                elif debug:
                    print(f"       ‚ùå Could not parse")
        
        # If no candidates found, try looking specifically in the scale bar area
        if not scale_candidates and debug:
            print("   ‚ö†Ô∏è No scale text found in standard search, trying focused search...")
            # Try searching specifically in the bottom-right area where scale bars typically are
            scale_region = bottom_region[:, int(width * 0.6):]
            try:
                results_focused = reader.readtext(scale_region, detail=1, text_threshold=0.3)
                if results_focused:
                    print(f"   Found {len(results_focused)} texts in focused search:")
                    for (bbox, text, confidence) in results_focused:
                        print(f"     '{text}' (confidence: {confidence:.3f})")
            except:
                pass
        
        # Select the best scale candidate
        if scale_candidates:
            # Sort by validation score and confidence
            scale_candidates.sort(key=lambda x: (x['validation_score'], x['confidence']), reverse=True)
            
            if debug:
                print(f"   Selected best candidate from {len(scale_candidates)} options")
            
            return scale_candidates[0]
    
    except Exception as e:
        if debug:
            print(f"‚ùå EasyOCR failed: {e}")
    
    return None

def is_sem_parameter_text(text: str) -> bool:
    """
    Filter out SEM imaging parameter text that might contain numbers.
    """
    
    text_lower = text.lower().strip()
    
    # Common SEM parameter indicators
    sem_indicators = [
        'kv',      # Voltage: "12.0 kV"
        'etd',     # Detector type
        'wd',      # Working distance
        'spot',    # Spot size
        'pressure', # Chamber pressure
        'temp',    # Temperature
        'mag',     # Magnification
        'det',     # Detector
        'hv',      # High voltage
        'hfov',    # Horizontal field of view
        'vfov',    # Vertical field of view
        'se',      # Secondary electron
        'bse',     # Backscattered electron
        'acc',     # Acceleration
        'mode',    # Imaging mode
        'scan',    # Scan rate/type
        'dwell',   # Dwell time
        'x',       # Magnification like "400x" - but only if followed by space or at end
    ]
    
    # Check for SEM parameter patterns
    for indicator in sem_indicators:
        # Special handling for 'x' - only filter if it's like "400x" not "pixel"
        if indicator == 'x':
            if re.search(r'\d+x\s*$', text_lower) or re.search(r'\d+x\s+', text_lower):
                return True
        elif indicator in text_lower:
            return True
    
    # Check for parameter patterns with values
    # Pattern: number + unit that's not a scale unit
    param_patterns = [
        r'\d+\.?\d*\s*kv',          # Voltage
        r'\d+\.?\d*\s*mm\s+wd',     # Working distance with WD
        r'\d+\.?\d*\s*pa',          # Pressure in Pascals
        r'\d+\.?\d*\s*torr',        # Pressure in Torr
        r'\d+\.?\d*\s*deg',         # Degrees
        r'\d+\.?\d*\s*¬∞',           # Degrees symbol
        r'\d+x\s*mag',              # Magnification
    ]
    
    for pattern in param_patterns:
        if re.search(pattern, text_lower):
            return True
    
    # Check for complex parameter strings (multiple values/units)
    # More than 2 numbers usually indicates parameters
    numbers = re.findall(r'\d+\.?\d*', text)
    if len(numbers) > 2:
        return True
    
    # Check for very long text (parameters are usually long)
    if len(text.strip()) > 30:  # Scale text is usually short
        return True
    
    # Check for multiple units in one text (indicates parameters)
    unit_count = 0
    units = ['kv', 'mm', 'pa', 'torr', 'deg', '¬∞', 'mbar', 'v', 'a']
    for unit in units:
        if unit in text_lower:
            unit_count += 1
    if unit_count > 1:
        return True
    
    return False

def validate_scale_text_context(text: str, bbox: np.ndarray, image_shape: Tuple[int, int]) -> float:
    """
    Validate that this text is likely a scale bar based on context.
    Returns a score from 0.0 (unlikely) to 1.0 (very likely).
    """
    
    score = 0.0
    height, width = image_shape
    
    # Position scoring - scale bars are usually in bottom-right or bottom-center
    center_x = np.mean(bbox[:, 0])
    center_y = np.mean(bbox[:, 1])
    
    # Prefer right side of image
    x_ratio = center_x / width
    if x_ratio > 0.7:  # Right side
        score += 0.3
    elif x_ratio > 0.4:  # Center-right area
        score += 0.2
    elif x_ratio > 0.3:  # Center area
        score += 0.1
    
    # Prefer bottom area
    y_ratio = center_y / height
    if y_ratio > 0.85:  # Very bottom
        score += 0.3
    elif y_ratio > 0.7:  # Lower area
        score += 0.2
    
    # Text format scoring
    text_clean = text.strip().lower()
    
    # Prefer simple scale text formats
    # Complete pattern: number + unit
    if re.match(r'^\d+\.?\d*\s*[Œº¬µu]?m$', text_clean):  # "300.0Œºm" format
        score += 0.4
    elif re.match(r'^\d+\.?\d*\s*[Œº¬µu]m', text_clean):  # "300Œºm" with possible extra
        score += 0.3
    elif re.match(r'^\d+\.?\d*\s*[a-z]{1,3}$', text_clean):  # Simple number+unit
        score += 0.2
    
    # Prefer reasonable scale values
    numbers = re.findall(r'\d+\.?\d*', text_clean)
    if numbers:
        try:
            value = float(numbers[0])
            if 50 <= value <= 2000:  # Typical SEM scale range in Œºm
                score += 0.2
            elif 10 <= value <= 5000:  # Extended range
                score += 0.1
            elif 0.1 <= value <= 10:  # Might be mm
                score += 0.1
        except:
            pass
    
    # Penalize if text is too long (scale bars are usually concise)
    if len(text_clean) > 15:
        score -= 0.1
    
    # Penalize if there are multiple separate numbers
    if len(numbers) > 1:
        score -= 0.2
    
    return max(0.0, min(1.0, score))  # Clamp between 0 and 1

def find_split_scale_bar(image: np.ndarray, text_info: Dict, debug=False) -> int:
    """
    Find the complete scale bar that is split by the text overlay.
    
    Strategy:
    1. Look for horizontal lines at the text's vertical middle
    2. Find segments to the left and right of the text
    3. Calculate total span: left_segment + text_width + right_segment
    
    Key insight: The scale bar line runs through the middle height of the text
    """
    
    # Text position info
    text_center_x = text_info['center_x']
    text_center_y = text_info['center_y']
    text_min_x = text_info['min_x']
    text_max_x = text_info['max_x']
    text_width = text_info['width']
    text_height = text_info['height']
    
    # The scale bar line should be at the vertical middle of the text
    line_y_target = text_center_y
    
    if debug:
        print(f"   Text spans from x={text_min_x} to x={text_max_x} (width={text_width})")
        print(f"   Text height: {text_height}, center_y: {text_center_y}")
        print(f"   Looking for scale line at y‚âà{line_y_target}")
    
    # Create search regions to left and right of text
    search_margin = 300  # Look further from text
    y_tolerance = 8      # Look within 8 pixels of text center vertically (tight)
    
    height, width = image.shape
    
    # Left search region
    left_x1 = max(0, text_min_x - search_margin)
    left_x2 = text_min_x - 5  # Small gap to avoid text artifacts
    search_y1 = max(0, line_y_target - y_tolerance)
    search_y2 = min(height, line_y_target + y_tolerance)
    
    left_region = image[search_y1:search_y2, left_x1:left_x2]
    
    # Right search region  
    right_x1 = text_max_x + 5  # Small gap to avoid text artifacts
    right_x2 = min(width, text_max_x + search_margin)
    
    right_region = image[search_y1:search_y2, right_x1:right_x2]
    
    if debug:
        print(f"   Left search region: {left_region.shape} at x=[{left_x1}:{left_x2}], y=[{search_y1}:{search_y2}]")
        print(f"   Right search region: {right_region.shape} at x=[{right_x1}:{right_x2}], y=[{search_y1}:{search_y2}]")
    
    # Find horizontal lines in each region
    left_lines = find_horizontal_lines_in_region(left_region, debug=debug)
    right_lines = find_horizontal_lines_in_region(right_region, debug=debug)
    
    if debug:
        print(f"   Found {len(left_lines)} left segments, {len(right_lines)} right segments")
    
    # Strategy 1: Try to match left and right segments that align horizontally
    best_span = 0
    best_details = None
    
    for left_line in left_lines:
        for right_line in right_lines:
            # Check if they're at similar Y positions (aligned)
            left_y_global = search_y1 + left_line['center_y']
            right_y_global = search_y1 + right_line['center_y']
            
            if abs(left_y_global - right_y_global) <= 3:  # Within 3 pixels vertically
                # Calculate total span from leftmost to rightmost point
                left_start_global = left_x1 + left_line['start_x']
                right_end_global = right_x1 + right_line['end_x']
                
                total_span = right_end_global - left_start_global
                
                # Score this combination
                score = score_line_pair(left_line, right_line, text_info)
                
                if debug:
                    print(f"     Candidate pair: left={left_line['length']}px, right={right_line['length']}px")
                    print(f"       Y alignment: left={left_y_global}, right={right_y_global}")
                    print(f"       Total span: {total_span}px, score: {score:.3f}")
                
                if score > 0.4 and total_span > best_span and 150 <= total_span <= 800:
                    best_span = total_span
                    best_details = {
                        'left_line': left_line,
                        'right_line': right_line,
                        'left_length': left_line['length'],
                        'right_length': right_line['length'],
                        'text_width': text_width,
                        'total_span': total_span,
                        'score': score,
                        'method': 'paired_segments'
                    }
    
    # Strategy 2: If no good pair found, try individual segments plus text width
    if best_span == 0:
        if debug:
            print("   No aligned pairs found, trying individual segments...")
        
        # Find the best individual lines
        best_left = max(left_lines, key=lambda x: x['score']) if left_lines else None
        best_right = max(right_lines, key=lambda x: x['score']) if right_lines else None
        
        if best_left and best_right:
            # Calculate span: left_segment + text_width + right_segment
            # This assumes the segments connect directly to the text
            estimated_span = best_left['length'] + text_width + best_right['length']
            
            if debug:
                print(f"   Reconstructed span: {best_left['length']} + {text_width} + {best_right['length']} = {estimated_span}")
            
            if 150 <= estimated_span <= 800:
                best_span = estimated_span
                best_details = {
                    'left_length': best_left['length'],
                    'right_length': best_right['length'],
                    'text_width': text_width,
                    'total_span': estimated_span,
                    'method': 'reconstructed'
                }
        
        elif best_left or best_right:
            # Use proportional estimation if we only have one side
            single_line = best_left or best_right
            # Assume the visible line is roughly 35-45% of total span
            estimated_total = int(single_line['length'] / 0.4)
            
            if debug:
                print(f"   Single-side estimation: {single_line['length']} / 0.4 = {estimated_total}")
            
            if 150 <= estimated_total <= 800:
                best_span = estimated_total
                best_details = {
                    'visible_length': single_line['length'],
                    'total_span': estimated_total,
                    'method': 'single_side_proportion'
                }
    
    # Strategy 3: Fallback to scan for any horizontal structures near text
    if best_span == 0:
        if debug:
            print("   Trying fallback: scan for horizontal structures...")
        
        fallback_span = scan_for_horizontal_structures(image, text_info, debug)
        if fallback_span > 0:
            best_span = fallback_span
            best_details = {
                'total_span': fallback_span,
                'method': 'fallback_scan'
            }
    
    if best_span > 0 and debug:
        print(f"‚úÖ Best span found: {best_span} pixels")
        if best_details:
            print(f"   Method: {best_details.get('method', 'unknown')}")
            if 'left_length' in best_details and 'right_length' in best_details:
                print(f"   Components: left={best_details['left_length']}px + text={best_details['text_width']}px + right={best_details['right_length']}px")
    
    return best_span

def scan_for_horizontal_structures(image: np.ndarray, text_info: Dict, debug=False) -> int:
    """
    Fallback method: scan the entire scale region for horizontal structures.
    """
    
    # Look in the bottom 20% of the image
    height, width = image.shape
    scale_region = image[int(height * 0.8):, :]
    
    # Find all horizontal structures
    structures = []
    
    for threshold in [250, 230, 200, 180]:
        _, binary = cv2.threshold(scale_region, threshold, 255, cv2.THRESH_BINARY)
        
        # Enhance horizontal structures
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 1))
        horizontal = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
        
        # Connect nearby segments
        connect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (10, 3))
        connected = cv2.morphologyEx(horizontal, cv2.MORPH_CLOSE, connect_kernel)
        
        contours, _ = cv2.findContours(connected, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        for contour in contours:
            x, y, w, h = cv2.boundingRect(contour)
            if w >= 100 and w / h >= 8:  # Reasonable horizontal structure
                structures.append(w)
    
    if structures:
        # Use the most common reasonable structure length
        reasonable = [s for s in structures if 150 <= s <= 600]
        if reasonable:
            from collections import Counter
            structure_counts = Counter(reasonable)
            best_structure = structure_counts.most_common(1)[0][0]
            
            if debug:
                print(f"   Fallback found: {best_structure} pixels")
            
            return best_structure
    
    return 0

def find_horizontal_lines_in_region(region: np.ndarray, debug=False) -> List[Dict]:
    """
    Find horizontal line segments in a region.
    """
    
    if region.size == 0:
        return []
    
    lines = []
    
    # Try multiple thresholds to catch different line intensities
    for threshold in [250, 240, 220, 200, 180]:
        _, binary = cv2.threshold(region, threshold, 255, cv2.THRESH_BINARY)
        
        # Use horizontal morphological opening to enhance horizontal structures
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (10, 1))
        horizontal = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
        
        # Find contours
        contours, _ = cv2.findContours(horizontal, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        for contour in contours:
            x, y, w, h = cv2.boundingRect(contour)
            
            # Filter for horizontal line-like shapes
            if w >= 15 and h <= 8 and w / h >= 3:
                area = cv2.contourArea(contour)
                fill_ratio = area / (w * h) if w * h > 0 else 0
                
                # Score the line quality
                length_score = min(1.0, w / 100)  # Longer is better
                aspect_score = min(1.0, (w / h) / 10)  # More horizontal is better
                fill_score = fill_ratio  # More filled is better
                
                score = (length_score + aspect_score + fill_score) / 3
                
                lines.append({
                    'start_x': x,
                    'end_x': x + w,
                    'center_x': x + w // 2,
                    'center_y': y + h // 2,
                    'length': w,
                    'thickness': h,
                    'area': area,
                    'fill_ratio': fill_ratio,
                    'score': score,
                    'threshold': threshold
                })
    
    # Remove duplicates (same line detected at different thresholds)
    unique_lines = []
    for line in lines:
        is_duplicate = False
        for existing in unique_lines:
            # Check if lines overlap significantly
            x_overlap = max(0, min(line['end_x'], existing['end_x']) - max(line['start_x'], existing['start_x']))
            if x_overlap > 0.8 * min(line['length'], existing['length']):
                # Keep the better scoring line
                if line['score'] > existing['score']:
                    unique_lines.remove(existing)
                    unique_lines.append(line)
                is_duplicate = True
                break
        
        if not is_duplicate:
            unique_lines.append(line)
    
    # Sort by score
    unique_lines.sort(key=lambda x: x['score'], reverse=True)
    
    if debug and unique_lines:
        print(f"     Found {len(unique_lines)} horizontal lines:")
        for i, line in enumerate(unique_lines[:3]):  # Show top 3
            print(f"       {i+1}. Length: {line['length']}px, score: {line['score']:.3f}")
    
    return unique_lines

def score_line_pair(left_line: Dict, right_line: Dict, text_info: Dict) -> float:
    """
    Score how well a pair of lines matches as a split scale bar.
    """
    
    score = 0.0
    
    # Length similarity (both sides should be roughly similar)
    length_diff = abs(left_line['length'] - right_line['length'])
    max_length = max(left_line['length'], right_line['length'])
    length_similarity = 1.0 - (length_diff / max_length) if max_length > 0 else 0
    score += 0.3 * length_similarity
    
    # Y alignment (should be at same height)
    y_diff = abs(left_line['center_y'] - right_line['center_y'])
    y_alignment = max(0, 1.0 - y_diff / 10)  # Perfect if within 10 pixels
    score += 0.3 * y_alignment
    
    # Individual line quality
    line_quality = (left_line['score'] + right_line['score']) / 2
    score += 0.4 * line_quality
    
    return score

def parse_scale_text_flexible(text: str) -> Optional[Dict]:
    """
    Enhanced flexible parser that handles all OCR variations and decimal formats.
    """
    
    # Clean text but preserve important characters
    text_clean = text.strip()
    
    if not text_clean:
        return None
    
    # Try multiple number extraction patterns
    number_patterns = [
        r'(\d+\.\d+)',           # Decimal: 300.0
        r'(\d+,\d+)',            # European decimal: 300,0  
        r'(\d+\.?\d*)',          # Regular: 300 or 300.5
        r'(\d+)',                # Integer only: 300
    ]
    
    value = None
    number_match = None
    for pattern in number_patterns:
        match = re.search(pattern, text_clean)
        if match:
            try:
                # Handle both . and , as decimal separators
                number_str = match.group(1).replace(',', '.')
                value = float(number_str)
                number_match = match
                break
            except:
                continue
    
    if value is None or number_match is None:
        return None
    
    # Enhanced unit detection with more OCR error patterns
    after_number = text_clean[number_match.end():].strip()
    
    # Create both original and lowercase versions for comparison
    after_lower = after_number.lower()
    after_original = after_number
    
    # Debug the unit detection
    unit_found = None
    micrometers = None
    
    # Micrometers - check for various OCR interpretations
    micrometer_patterns = [
        # Standard patterns
        'Œºm', '¬µm', 'um', 
        # Common OCR errors
        'jm', 'pm', 'qm', 'om', '≈´m', '√ªm', '≈©m',
        # With dots/periods
        'Œºm.', 'um.', 'jm.', '.0Œºm', '.0um',
        # Spaced versions
        'Œº m', 'u m', 'j m',
        # Character recognition errors
        'ŒºœÄŒπ', 'Œºm', 'ŒºŒ∑', 'ŒºŒº',
        # Number continuation patterns (300.0Œºm might be read as two parts)
        '.0Œºm', '.0um', '.ojm', '.oum',
        # Single characters that might be 'm' after a number
        'm', 'n', 'œÄ', 'Œ∑'
    ]
    
    # Check for micrometer patterns
    for pattern in micrometer_patterns:
        if pattern in after_lower or pattern in after_original:
            unit_found = 'Œºm'
            micrometers = value
            break
    
    # If no micrometer pattern found, check other units
    if unit_found is None:
        if any(nm_pattern in after_lower for nm_pattern in ['nm', 'Œ∑m', 'Œ∑œÄŒπ']):
            unit_found = 'nm'
            micrometers = value * 0.001
        elif any(mm_pattern in after_lower for mm_pattern in ['mm', 'œÄœÄŒπ', 'mm.']):
            # But NOT if it's part of a parameter like "15.5 mm WD"
            if 'wd' not in after_lower and 'working' not in after_lower:
                unit_found = 'mm'  
                micrometers = value * 1000
        elif any(cm_pattern in after_lower for cm_pattern in ['cm', 'cŒ∑']):
            unit_found = 'cm'
            micrometers = value * 10000
    
    # Smart default: if we have a reasonable SEM scale value but no clear unit
    if unit_found is None:
        if 50 <= value <= 2000:  # Typical SEM micrometer range
            unit_found = 'Œºm'
            micrometers = value
        elif 50000 <= value <= 2000000:  # Might be nanometers interpreted as number
            unit_found = 'nm'
            micrometers = value * 0.001
        elif 0.05 <= value <= 2:  # Might be millimeters
            unit_found = 'mm'
            micrometers = value * 1000
    
    # Final validation
    if unit_found and micrometers is not None:
        # Expanded range for different types of SEM images
        if 1 <= micrometers <= 10000:  # 1Œºm to 10mm range (very generous)
            return {
                'value': value,
                'unit': unit_found,
                'micrometers': micrometers,
                'original_text': text_clean,
                'after_number': after_number  # For debugging
            }
    
    return None

# Compatibility functions
def detect_scale_bar(image_input, debug=True, **kwargs) -> Dict:
    """Main detection function with split-bar awareness."""
    return detect_scale_bar_split_aware(image_input, debug=debug, **kwargs)

def detect_scale_factor_only(image_input, **kwargs) -> float:
    """Return just the scale factor."""
    result = detect_scale_bar_split_aware(image_input, **kwargs)
    return result['micrometers_per_pixel']

class ScaleBarDetector:
    """Compatibility class."""
    def __init__(self, **kwargs):
        self.debug = kwargs.get('debug', True)
    
    def detect_scale_bar(self, image: np.ndarray) -> Dict:
        return detect_scale_bar_split_aware(image, debug=self.debug)

def manual_scale_calibration(bar_length_pixels: int, bar_length_micrometers: float) -> float:
    """Manual calibration."""
    return bar_length_micrometers / bar_length_pixels

def debug_ocr_for_image(image_input, debug=True):
    """
    Debug OCR specifically for problematic images.
    """
    
    # Handle image loading
    if isinstance(image_input, str):
        image = cv2.imread(image_input, cv2.IMREAD_GRAYSCALE)
        if image is None:
            print(f"‚ùå Could not load image: {image_input}")
            return
    else:
        image = image_input
    
    if not EASYOCR_AVAILABLE:
        print("‚ùå EasyOCR not available")
        return
    
    print(f"üîç DEBUG OCR for image: {image.shape}")
    
    try:
        reader = easyocr.Reader(['en'], verbose=False)
        
        # Try different regions
        regions_to_test = [
            ("Full image", image),
            ("Bottom 30%", image[int(image.shape[0] * 0.7):, :]),
            ("Bottom 20%", image[int(image.shape[0] * 0.8):, :]),
            ("Bottom 15%", image[int(image.shape[0] * 0.85):, :]),
        ]
        
        for region_name, region in regions_to_test:
            print(f"\nüìã Testing {region_name}: {region.shape}")
            
            results = reader.readtext(region, detail=1)
            print(f"   Found {len(results)} text elements:")
            
            for i, (bbox, text, confidence) in enumerate(results):
                print(f"     {i+1}. '{text}' (confidence: {confidence:.3f})")
                
                # Check if it's a SEM parameter
                if is_sem_parameter_text(text.strip()):
                    print(f"        ‚ö†Ô∏è SEM parameter detected - would be filtered")
                
                # Try to parse each text
                scale_info = parse_scale_text_flexible(text.strip())
                if scale_info:
                    print(f"        ‚úÖ PARSED: {scale_info['value']} {scale_info['unit']} = {scale_info['micrometers']} Œºm")
                    print(f"        Debug info: after_number='{scale_info.get('after_number', 'N/A')}'")
                else:
                    print(f"        ‚ùå Could not parse")
                    
                    # Show detailed parsing attempt
                    print(f"        Debug: text='{text.strip()}'")
                    number_match = re.search(r'(\d+\.?\d*)', text.strip())
                    if number_match:
                        after_number = text.strip()[number_match.end():].strip()
                        print(f"        Debug: number='{number_match.group(1)}', after='{after_number}'")
                    else:
                        print(f"        Debug: No number found in text")
        
        print(f"\nüéØ Full OCR analysis complete")
        
    except Exception as e:
        print(f"‚ùå OCR debug failed: {e}")

# Test function
def test_single_problematic_image(image_path):
    """Test a single problematic image with detailed debugging."""
    
    print(f"üß™ DETAILED DEBUG for: {image_path}")
    print("=" * 60)
    
    # First, run OCR debug
    debug_ocr_for_image(image_path)
    
    # Then run the full detection
    print(f"\nüìè Running full scale detection...")
    result = detect_scale_bar_split_aware(image_path, debug=True)
    
    if result['scale_detected']:
        print(f"üéâ SUCCESS: {result['micrometers_per_pixel']:.4f} Œºm/pixel")
    else:
        print(f"‚ùå FAILED: {result.get('error')}")
    
    return result

def test_split_bar_detection():
    """Test the split-bar aware detection on all images in the folder."""
    
    print("üß™ TESTING SPLIT-BAR AWARE DETECTION - ALL IMAGES")
    print("=" * 60)
    
    from pathlib import Path
    
    # Find test images in multiple possible locations
    possible_dirs = [
        Path("sample_images"),
        Path("../sample_images"),
        Path("./"),  # Current directory
        Path("images"),  # Alternative name
        Path("test_images")  # Alternative name
    ]
    
    sample_dir = None
    for dir_path in possible_dirs:
        if dir_path.exists():
            # Check if it contains image files
            image_extensions = ['.jpg', '.jpeg', '.png', '.tif', '.tiff', '.bmp']
            has_images = any(
                list(dir_path.glob(f'*{ext}')) + list(dir_path.glob(f'*{ext.upper()}'))
                for ext in image_extensions
            )
            if has_images:
                sample_dir = dir_path
                break
    
    if sample_dir is None:
        print("‚ùå No directory with image files found")
        print("   Searched in:", [str(d) for d in possible_dirs])
        return False
    
    print(f"üìÅ Found images in: {sample_dir.absolute()}")
    
    # Get ALL image files
    image_files = []
    image_extensions = ['.jpg', '.jpeg', '.png', '.tif', '.tiff', '.bmp', '.gif']
    
    for ext in image_extensions:
        image_files.extend(sample_dir.glob(f'*{ext}'))
        image_files.extend(sample_dir.glob(f'*{ext.upper()}'))
    
    # Remove duplicates and sort
    image_files = sorted(set(image_files))
    
    if not image_files:
        print("‚ùå No image files found")
        print(f"   Looked for extensions: {image_extensions}")
        return False
    
    print(f"üîç Found {len(image_files)} image files:")
    for i, img_file in enumerate(image_files, 1):
        print(f"   {i:2d}. {img_file.name}")
    
    print("\n" + "=" * 60)
    print("PROCESSING ALL IMAGES")
    print("=" * 60)
    
    successful = 0
    results_summary = []
    
    for i, image_path in enumerate(image_files, 1):
        print(f"\n[{i}/{len(image_files)}] üì∏ Processing: {image_path.name}")
        print("-" * 50)
        
        try:
            result = detect_scale_bar_split_aware(str(image_path), debug=True)
            
            if result['scale_detected']:
                scale_factor = result['micrometers_per_pixel']
                total_span = result['bar_length_pixels']
                scale_value = result['scale_info']['value']
                scale_unit = result['scale_info']['unit']
                
                print(f"üéâ SUCCESS: {scale_factor:.4f} Œºm/pixel")
                print(f"   Total span: {total_span} pixels")
                print(f"   Scale value: {scale_value} {scale_unit}")
                
                results_summary.append({
                    'filename': image_path.name,
                    'success': True,
                    'scale_factor': scale_factor,
                    'total_span': total_span,
                    'scale_value': scale_value,
                    'scale_unit': scale_unit,
                    'method': result.get('detection_method', 'unknown')
                })
                
                successful += 1
            else:
                error_msg = result.get('error', 'Unknown error')
                print(f"‚ùå FAILED: {error_msg}")
                
                results_summary.append({
                    'filename': image_path.name,
                    'success': False,
                    'error': error_msg
                })
        
        except Exception as e:
            print(f"üí• ERROR: {str(e)}")
            results_summary.append({
                'filename': image_path.name,
                'success': False,
                'error': f'Exception: {str(e)}'
            })
    
    # Final summary
    print("\n" + "=" * 60)
    print("FINAL RESULTS SUMMARY")
    print("=" * 60)
    
    print(f"üìä Overall Success Rate: {successful}/{len(image_files)} ({successful/len(image_files)*100:.1f}%)")
    
    if successful > 0:
        successful_results = [r for r in results_summary if r['success']]
        scale_factors = [r['scale_factor'] for r in successful_results]
        
        print(f"\nüìè Scale Factor Statistics:")
        print(f"   Range: {min(scale_factors):.4f} - {max(scale_factors):.4f} Œºm/pixel")
        print(f"   Mean: {sum(scale_factors)/len(scale_factors):.4f} Œºm/pixel")
        print(f"   Values: {[f'{sf:.4f}' for sf in sorted(scale_factors)]}")
        
        print(f"\n‚úÖ Successful Images:")
        for result in successful_results:
            print(f"   {result['filename']}: {result['scale_factor']:.4f} Œºm/px ({result['scale_value']} {result['scale_unit']})")
    
    # Show failures
    failed_results = [r for r in results_summary if not r['success']]
    if failed_results:
        print(f"\n‚ùå Failed Images ({len(failed_results)}):")
        for result in failed_results:
            print(f"   {result['filename']}: {result['error']}")
    
    # Save results to file
    try:
        import json
        from datetime import datetime
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = sample_dir / f"scale_detection_results_{timestamp}.json"
        
        with open(results_file, 'w') as f:
            json.dump({
                'timestamp': timestamp,
                'total_images': len(image_files),
                'successful': successful,
                'success_rate': successful/len(image_files)*100,
                'results': results_summary
            }, f, indent=2)
        
        print(f"\nüíæ Results saved to: {results_file}")
        
    except Exception as e:
        print(f"\n‚ö†Ô∏è Could not save results file: {e}")
    
    print(f"\nüéØ BATCH TEST COMPLETE!")
    
    if successful >= len(image_files) * 0.8:  # 80% success rate
        print("üéâ Excellent! Split-bar detection is working very well!")
        return True
    elif successful >= len(image_files) * 0.6:  # 60% success rate
        print("üëç Good! Split-bar detection is working reasonably well!")
        return True
    else:
        print("üòû Needs improvement - success rate too low")
        return False

if __name__ == "__main__":
    test_split_bar_detection()